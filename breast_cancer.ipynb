{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast cancer detection\n",
    "\n",
    "## Scenario\n",
    "You have been hired as a Data Scientist by a biotech startup that is developing sensors for early detection of breast cancer. During the startup's recent fundraising round, several investors asked whether the sensor performance could be further improved through the use of Deep Learning, \"big data\" and other such buzzwords. Which is where you come in.\n",
    "\n",
    "Before diving into more complicated techniques (which require significant more time and resources), you start by evaluating more common Machine Learning methods on public (and readily available) datasets. The goal is to provide a benchmark on baseline performance and to determine whether the concept is valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# classifiers\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# make the code compatible with both Python 2 and 3\n",
    "from __future__ import print_function, division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breast cancer data\n",
    "We'll load the breast cancer data that comes bundle with scikit-learn.\n",
    "\n",
    "More info for the dataset can be found on the UCI Machine Learning Repository: https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load breast cancer data\n",
    "# - X = (n_samples, n_features) feature array\n",
    "# - y = (n_samples) target vector\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# get summary stats\n",
    "print(\"# of samples:\", X.shape[0])\n",
    "print(\"# of features:\", X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What do you know about this dataset? Do the specifics of the dataset help you any way (e.g. narrowing down the type of models to consider)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data\n",
    "Before going further, we should split the data into training and testing sets so we can use some supervised learning methods. The idea is to learn a model using the data only in the training set, and then evaluate the data on a new (untouched/unseen) testing set.\n",
    "\n",
    "There are many ways to split data into training and testing. To start, we'll simply use a function from scikit-learn that splits the data randomly into specified sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into training and testing sets (60/40 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "# check the sizes of the training and testing sets\n",
    "\n",
    "# inspect the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a predictive model\n",
    "We'll start by training some common classification methods to predict whether a patient will get breast cancer. The simplest method to start with is Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# train the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# check the accuracy [-] of the trained model\n",
    "print(\"Score (train): {:.3f}\".format(model.score(X_train, y_train)))\n",
    "print(\"Score (test):  {:.3f}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** How did the model perform? Is the performance \"good enough\" or do we need to go further? How could we decide (since we are not experts on breast cancer detection in this scenario)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of pre-processing \n",
    "Let's evaluate the effect(s) of pre-processing using some standard techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scale the data so it has a mean=0 and variance=1\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# split the data into training and testing sets (60/40 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "\n",
    "# train a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Score (train): {:.3f}\".format(model.score(X_train, y_train)))\n",
    "print(\"Score (test):  {:.3f}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What was the effect(s) of the pre-processing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing classification methods\n",
    "Now let's try comparing the performance of a range of classification methods. Note that predictive accuracy of the methods is only one metric for comparing the performance of the methods. For example, keeping training times down may be preferrable to a 1-2% increase in accuracy.\n",
    "\n",
    "In the code cells below, try out some other common classification methods (which we've already imported at the start of the notebook):\n",
    "- Ridge Classifier\n",
    "- SGDClassifier\n",
    "- SVC\n",
    "- NuSVC\n",
    "- MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ridge Classifier\n",
    "model = RidgeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Score (train): {:.3f}\".format(model.score(X_train, y_train)))\n",
    "print(\"Score (test):  {:.3f}\".format(model.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SGD Classifier (SGD = Stochastic Gradient Descent)\n",
    "model = ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Support Vector Classifier (SVC)\n",
    "model = ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NuSVC\n",
    "model = ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LinearSVC\n",
    "model = ???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MLPClassifier (MLP = Multi-Layer Perceptron)\n",
    "model = ???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What do the results seem to indicate about the performance of the various methods? Are there any limitations to the analysis (e.g. should you try other methods or other data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps\n",
    "What should you do next? Do you have enough results to draw any conclusions or recommend something to your company?\n",
    "\n",
    "Going back to the main point, do these results answer the question of whether your company should be integrating topics such as Deep Learning or \"big data\"? Why or why not? If no, what else is needed before you can answer the question?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
